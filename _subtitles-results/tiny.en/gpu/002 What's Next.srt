1
00:00:00,000 --> 00:00:07,000
You have completed this course. Assuming you have watched a whole lecture in the given order.

2
00:00:07,000 --> 00:00:12,000
So what is next? Sometimes people will ask me my private message that,

3
00:00:12,000 --> 00:00:17,000
hey we know can you suggest what I should learn next? Something like that.

4
00:00:17,000 --> 00:00:27,000
I have created few courses and I can explain on what basis I created them so that you can pick the next topic which interest you.

5
00:00:27,000 --> 00:00:31,000
Let's consider an application with the microservices or the detector here.

6
00:00:31,000 --> 00:00:35,000
We have a bunch of services talking to one another like this.

7
00:00:35,000 --> 00:00:41,000
Let's say we are receiving hundreds of thousands of fragments per minute here.

8
00:00:41,000 --> 00:00:49,000
How can we process all these records more efficiently without launching or before horizontally scaling another instance?

9
00:00:49,000 --> 00:00:54,000
How can we make use of the given CPU or memory properly?

10
00:00:54,000 --> 00:01:00,000
Here we have two options. One is using Java virtual thread.

11
00:01:00,000 --> 00:01:05,000
Java finally after many years they have introduced virtual threads.

12
00:01:05,000 --> 00:01:17,000
Using virtual threads we write synchronous blocking style code but Java will do non blocking I.O. for us to process request more efficiently.

13
00:01:17,000 --> 00:01:20,000
I have a course on that. Check that out.

14
00:01:20,000 --> 00:01:26,000
Another option is developing reactive microservice with the spring vertex.

15
00:01:26,000 --> 00:01:35,000
The whole reactive paradigm is kind of new different style of programming and reactive programming enables us to do stream based communication.

16
00:01:35,000 --> 00:01:41,000
It is very hard to achieve with the synchronous blocking style code.

17
00:01:41,000 --> 00:01:52,000
So by using either of these options we can develop on single microservice which should be capable of processing breakfast more efficiently.

18
00:01:52,000 --> 00:01:56,000
Microservices are great. You see to develop test and deploy.

19
00:01:56,000 --> 00:02:03,000
But when we create multiple microservices and we are talking to one another like this there will be network latency.

20
00:02:03,000 --> 00:02:11,000
And it will affect the overall application processing time. How can we improve our backend communication?

21
00:02:11,000 --> 00:02:18,000
For the backend communication I can suggest three options. The one is GRPC created by Google.

22
00:02:18,000 --> 00:02:30,000
They have been using it for years. It uses protocol buffers and HTTP to reduce the network latency and improve the application throughput response time etc.

23
00:02:30,000 --> 00:02:41,000
So this is great. You can check this out. The next one is our socket. It's almost same as this GRPC but it's reactive style.

24
00:02:41,000 --> 00:02:47,000
This one is from Netflix. This is also great for the backend communication.

25
00:02:47,000 --> 00:02:57,000
If you like messaging for the asynchronous communication among microservices then we can also go with Kafka.

26
00:02:57,000 --> 00:03:16,000
We can compare these but before that let me clarify one thing. What is stream based communication? Traditionally our programming style or our application communication will be like client will send a contract request for which the server will give us one response back.

27
00:03:16,000 --> 00:03:27,000
This is how normally we do things. In the stream based communication it will be like two people talking to one another via phone.

28
00:03:27,000 --> 00:03:36,000
The client will call the server once the connection is established they can exchange information continuously.

29
00:03:36,000 --> 00:03:39,000
So we can design interactive application.

30
00:03:39,000 --> 00:03:45,000
Now we can compare the various technologies or options we have for the backend communication.

31
00:03:45,000 --> 00:03:58,000
If we want RESTful APIs we have two options. We can go with the Spring web for the Synchronous blocking style code and you will get request and response style programming but it's very hard to achieve stream based communication.

32
00:03:58,000 --> 00:04:04,000
If we want stream based communication along with the RESTful APIs then we can go with the Spring Web Flex.

33
00:04:04,000 --> 00:04:08,000
That is where the reactive programming shines.

34
00:04:08,000 --> 00:04:16,000
If we do not like REST for the backend communication things do not have to be RESTful actually.

35
00:04:16,000 --> 00:04:32,000
If we want low latency which I would highly encourage you to try these for the low latency communication we can have GRPC which will provide the request and response stream based communication etc.

36
00:04:32,000 --> 00:04:37,000
Then we have our socket same as this one but it's reactive.

37
00:04:37,000 --> 00:04:51,000
GRPC uses HTTP2 it's a network level 7 or socket is the custom protocol. It uses network level 5 so because of that in theory our socket should provide better performance compared to GRPC.

38
00:04:51,000 --> 00:04:55,000
However GRPC is not really tied to HTTP2.

39
00:04:55,000 --> 00:04:59,000
In the future they might change the protocol as well.

40
00:04:59,000 --> 00:05:06,000
So you can't go wrong with the either of these actually.

41
00:05:06,000 --> 00:05:12,000
Then we have Kafka for the completely even driven asynchronous communication.

42
00:05:12,000 --> 00:05:15,000
What about the communication with the front end?

43
00:05:15,000 --> 00:05:19,000
We have browser and mobile application etc talk to the backend.

44
00:05:19,000 --> 00:05:21,000
So what can we do here?

45
00:05:21,000 --> 00:05:27,000
I can suggest GraphQL here to avoid some of the challenges with the RESTful APIs and for efficient

46
00:05:27,000 --> 00:05:30,000
data retrieval.

47
00:05:30,000 --> 00:05:33,000
I have one dedicated course on GraphQL.

48
00:05:33,000 --> 00:05:34,000
Check that out.

49
00:05:34,000 --> 00:05:43,000
Particularly for mobile applications the GRPC and the R socket can also work great here as they are lightweight.

50
00:05:43,000 --> 00:05:49,000
Then what else I can do to improve my application throughput and reduce redundant work.

51
00:05:49,000 --> 00:06:00,000
We can also add radius which is high performance caching layer which provides super fast read write operations.

52
00:06:00,000 --> 00:06:06,000
We can add radius in our architecture to improve the application throughput and the response time.

53
00:06:06,000 --> 00:06:14,000
It can also provide features like PubSub messaging, distributed locking among microservices.

54
00:06:14,000 --> 00:06:17,000
We have to be careful when we design our application.

55
00:06:17,000 --> 00:06:20,000
Anything could happen in the network.

56
00:06:20,000 --> 00:06:25,000
One service might be very slow or one service might fail suddenly.

57
00:06:25,000 --> 00:06:30,000
The problem in one service should not propagate everywhere.

58
00:06:30,000 --> 00:06:37,000
It is always our responsibility to design our application to be highly resilient.

59
00:06:37,000 --> 00:06:44,000
There are some techniques design patterns we can follow to develop highly resilient application.

60
00:06:44,000 --> 00:06:50,000
I have documented that in this course microservices design patterns.

61
00:06:50,000 --> 00:06:54,000
Integration and resilient design patterns for microservices architecture.

62
00:06:54,000 --> 00:06:56,000
What about testing?

63
00:06:56,000 --> 00:06:58,000
I have a course on Selenium.

64
00:06:58,000 --> 00:07:02,000
The running automated U8 test via CICD pipeline.

65
00:07:02,000 --> 00:07:05,000
I use Jenkins in this course.

66
00:07:05,000 --> 00:07:08,000
You can check that out if you are interested.

67
00:07:08,000 --> 00:07:09,000
Okay.

68
00:07:09,000 --> 00:07:13,000
We have developed our application which is scalable, performant, etc.

69
00:07:13,000 --> 00:07:15,000
So, what about deployment?

70
00:07:15,000 --> 00:07:16,000
Service discovery.

71
00:07:16,000 --> 00:07:20,000
Here, I would suggest Docker and Kubernetes.

72
00:07:20,000 --> 00:07:23,000
Docker is a must know tool if you ask me.

73
00:07:23,000 --> 00:07:35,000
If you have never tried Docker, you will be amazed how it can increase our productivity by quickly spinning up the dependent application like databases or other applications for local development.

74
00:07:35,000 --> 00:07:40,000
It is easy to package deploy share applications via Docker.

75
00:07:40,000 --> 00:07:43,000
Kubernetes is for orchestration.

76
00:07:43,000 --> 00:07:52,000
For service discovery load balancing, rolling update, rollback, configuration management, another super important feature or to scaling.

77
00:07:52,000 --> 00:08:02,000
We definitely should not want to lock ourselves with one particular cloud provider like AWS or GCP, etc.

78
00:08:02,000 --> 00:08:10,000
Kubernetes is the way to develop applications which can provide multi-cloud support.

79
00:08:10,000 --> 00:08:13,000
I have one course on this Kubernetes.

80
00:08:13,000 --> 00:08:15,000
We will learn Kubernetes in the end.

81
00:08:15,000 --> 00:08:22,000
You will see in less than 10 minutes, we will run our application on GCP cloud without doing much.

82
00:08:22,000 --> 00:08:29,000
Auto-scaling service discovery, etc. will work just out of the box with a Kubernetes configuration.

83
00:08:29,000 --> 00:08:34,000
To quickly summarize, these are the topics for which I have courses for now.

84
00:08:34,000 --> 00:08:37,000
Pass this video if you want to review.

85
00:08:37,000 --> 00:08:44,000
Once you picked up the next topic, if you go to my profile in this E-DMA platform,

86
00:08:44,000 --> 00:08:48,000
then you should be able to find all the courses.

87
00:08:48,000 --> 00:08:51,000
Thanks and I'll see you soon.

